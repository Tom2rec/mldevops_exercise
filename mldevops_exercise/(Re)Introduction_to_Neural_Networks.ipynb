{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19hfXeGSNYwJ"
   },
   "source": [
    "# Lab 1 (Re)Introduction to Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "619REVu-NKv0"
   },
   "source": [
    "## I. Supervised Learning - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeDsNB9rODhV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQEywuuIOBcy"
   },
   "outputs": [],
   "source": [
    "class FashionMNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FashionMNISTModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3_IQOUoORlb"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOs3ZCM6TvMk",
    "outputId": "ec13516c-4ec9-4b77-a66e-3b6fcaae1815"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "epochs = 14\n",
    "lr = 1.0\n",
    "gamma = 0.7\n",
    "no_cuda = False\n",
    "no_mps = False\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = DataLoader(dataset1, batch_size)\n",
    "test_loader = DataLoader(dataset2, test_batch_size)\n",
    "\n",
    "model = FashionMNISTModel().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUVDG-HDPxas"
   },
   "source": [
    "## II. Unsupervised Learning - Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "sz7cpyUZk3CL",
    "outputId": "a1f27e41-4ed0-4415-ab55-4e66d68ccd25"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Building an linear encoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # 784 ==> 9\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "          torch.nn.Linear(28 * 28, 128),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(128, 64),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(64, 36),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(36, 18),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(18, 9)\n",
    "        )\n",
    "\n",
    "        # Building an linear decoder with Linear\n",
    "        # layer followed by Relu activation function\n",
    "        # The Sigmoid activation function\n",
    "        # outputs the value between 0 and 1\n",
    "        # 9 ==> 784\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "          torch.nn.Linear(9, 18),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(18, 36),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(36, 64),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(64, 128),\n",
    "          torch.nn.ReLU(True),\n",
    "          torch.nn.Linear(128, 28 * 28),\n",
    "          torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Data transformation and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the 28x28 images\n",
    "])\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        img = img.view(-1, 28*28)  # Flatten images to 784-dimensional vectors\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_loader)  # Average loss for the epoch\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plotting loss over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), losses, marker='o', color='b', label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualization function\n",
    "def show_images(original, reconstructed):\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(15, 4))\n",
    "    for i in range(10):\n",
    "        axes[0, i].imshow(original[i].reshape(28, 28), cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test and visualize reconstructed images\n",
    "test_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "images, _ = next(iter(test_loader))\n",
    "images = images.view(-1, 28*28)\n",
    "with torch.no_grad():\n",
    "    reconstructed_images = model(images)\n",
    "show_images(images.numpy(), reconstructed_images.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8woSA3rXRmj"
   },
   "source": [
    "## III. Usage of pre-trained models - Speech Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cXQE79tXVRt",
    "outputId": "963181f6-2bd1-4a40-e69b-a536e04a6230"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Ac_Nn9nofdB",
    "outputId": "0660b068-7b6c-428e-a97b-ee32612558d8"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.utils import download_asset\n",
    "\n",
    "SPEECH_FILE = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_7XwFitomsr",
    "outputId": "7a1a316b-053c-47ed-964c-979e2a23d6bf"
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "\n",
    "print(\"Sample Rate:\", bundle.sample_rate)\n",
    "\n",
    "print(\"Labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpvcX2fCo4bd",
    "outputId": "dcb52b3f-d2b8-4fa3-e445-ef2381a262d5"
   },
   "outputs": [],
   "source": [
    "model = bundle.get_model().to(device)\n",
    "\n",
    "print(model.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "a00tu3vIo7EA",
    "outputId": "ccf966c7-7f73-4002-aa6b-707d8ef42f38"
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-I1SS9bpBQ0"
   },
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "waveform = waveform.to(device)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-rIBuILpTMo"
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = model.extract_features(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ydLuatdvpVpW",
    "outputId": "d76330a0-e79c-4da8-9dc6-235ea57ad1f7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(features), 1, figsize=(16, 4.3 * len(features)))\n",
    "for i, feats in enumerate(features):\n",
    "    ax[i].imshow(feats[0].cpu(), interpolation=\"nearest\")\n",
    "    ax[i].set_title(f\"Feature from transformer layer {i+1}\")\n",
    "    ax[i].set_xlabel(\"Feature dimension\")\n",
    "    ax[i].set_ylabel(\"Frame (time-axis)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaQ_0XLzpa3v"
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = model(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "a9zbldEfpekr",
    "outputId": "003a6bdd-2ce5-4cc1-f04a-4cbed74645ab"
   },
   "outputs": [],
   "source": [
    "plt.imshow(emission[0].cpu().T, interpolation=\"nearest\")\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "print(\"Class labels:\", bundle.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJt4B7xspilK"
   },
   "outputs": [],
   "source": [
    "class GreedyCTCDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank = blank\n",
    "\n",
    "    def forward(self, emission: torch.Tensor) -> str:\n",
    "        \"\"\"Given a sequence emission over labels, get the best path string\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[num_seq, num_label]`.\n",
    "\n",
    "        Returns:\n",
    "          str: The resulting transcript\n",
    "        \"\"\"\n",
    "        indices = torch.argmax(emission, dim=-1)  # [num_seq,]\n",
    "        indices = torch.unique_consecutive(indices, dim=-1)\n",
    "        indices = [i for i in indices if i != self.blank]\n",
    "        return \"\".join([self.labels[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9JwoQtppkZf"
   },
   "outputs": [],
   "source": [
    "decoder = GreedyCTCDecoder(labels=bundle.get_labels())\n",
    "transcript = decoder(emission[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "X6yEwQHjpmRi",
    "outputId": "ba99dfd5-1f02-4b27-b876-be925aa60867"
   },
   "outputs": [],
   "source": [
    "print(transcript)\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_kqRkJB6sDVK"
   },
   "source": [
    "## IV - DIY Implementation - Artificial Neuron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9ZjnASzs_YW",
    "outputId": "556fad36-4ed9-426d-9d7a-980fbc55a580"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Layer size\n",
    "input_size = 2\n",
    "neuron_units = 10\n",
    "\n",
    "# Initialization\n",
    "x = np.random.randn(input_size, 1)\n",
    "weights = np.random.randn(neuron_units, input_size)\n",
    "biases = np.random.randn(neuron_units, 1)\n",
    "\n",
    "# Forward propagation\n",
    "activation = np.dot(weights, x) + biases\n",
    "y = ReLU(activation)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCU7BHqNuJbX"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GOAL: Build a ANN network with 2 layers\n",
    "* 1 layer:\n",
    "    * Input size: 10\n",
    "    * Output size, (aka. Neuron units; aka. layer size): 10\n",
    "    * Activation function (for all units): ReLU\n",
    "* 2 layer:\n",
    "    * Input size: (same as previous layer size, here: 10)\n",
    "    * Output size: 2\n",
    "    * Activation function: Softmax\n",
    "'''\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from collections import namedtuple\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Let's start with definitions of some activation functions\n",
    "# ReLU with derivative for the 1st layer\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def ReLU_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "# And Softmax for the 2nd layer\n",
    "# taken from https://stackoverflow.com/a/54977170\n",
    "\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z)\n",
    "    return e / np.sum(e, axis=1)\n",
    "\n",
    "def softmax_deriv(Z):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    Z_reshaped = Z.reshape(-1,1)\n",
    "    return np.diagflat(Z_reshaped) - np.dot(Z_reshaped, Z_reshaped.T)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Now, we need to somehowe randomly initizalize weights & biases.\n",
    "# Let's start with some preoparation, and define namedtuples for passing our network parameters\n",
    "Layer = namedtuple('Layer', ['weights', 'bias'])\n",
    "ANN = namedtuple('ANN', ['layer_1', 'layer_2'])\n",
    "\n",
    "# Then, for simplicity, let's initialize our network in naive way\n",
    "def init_ann_params(input_size: int, layer_1_size: int, layer_2_size: int) -> ANN:\n",
    "\n",
    "    # The weights matrix must have dimensions equal to sizes of the input and the number of neural units\n",
    "    weights_1 = np.random.rand(layer_1_size, input_size) - 0.5\n",
    "    # The bias vector will have a one parameter (the bias) for each neural unit\n",
    "    bias_1 = np.random.rand(layer_1_size, 1)- 0.5\n",
    "    # Let's pack it into our namedtuple\n",
    "    layer_1 = Layer(weights=weights_1, bias=bias_1)\n",
    "\n",
    "    # Let's do it analogously with 2nd layer.\n",
    "    # The input size must mach the size of the previous layer.\n",
    "    # The output size of this layer will be also the output size of the whole network.\n",
    "    weights_2 = np.random.rand(layer_2_size, layer_1_size)- 0.5\n",
    "    bias_2 = np.random.rand(layer_2_size, 1)- 0.5\n",
    "    layer_2 = Layer(weights=weights_2, bias=bias_2)\n",
    "\n",
    "    # Finally, return our fresh network, packed in our namedtuple\n",
    "    return ANN(layer_1, layer_2)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# It's time for Forward Propagation definition\n",
    "def forward_prop(ann: ANN, input_x: np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "\n",
    "    # Retrieve parameters for layer 1\n",
    "    layer_1 = ann.layer_1\n",
    "    # Using matrix notation multiply weights matrix by input vector and add bias vector for whole layer 1\n",
    "    sum_1 = layer_1.weights.dot( input_x ) + layer_1.bias\n",
    "    # Then use the sum vector (each element consist of sum for each neuron in this layer) as input for activation function\n",
    "    activate_1 = ReLU(sum_1)\n",
    "\n",
    "    # Moving forward to the 2nd layer - retrieve parameters\n",
    "    layer_2 = ann.layer_2\n",
    "    # Use the output of the 1st layer as input for the matrix calcucations in 2nd layer\n",
    "    sum_2 = layer_2.weights.dot( input_x ) + layer_2.bias\n",
    "    # Use the sum vector with the activation function of the last layer\n",
    "    activate_2 = softmax(sum_2)\n",
    "\n",
    "    # Return the sums and the activations results\n",
    "    # (The sums will be needed to calculate derivatives of the activation functions!)\n",
    "    return sum_1, activate_1, sum_2, activate_2\n",
    "\n",
    "# =============================== #\n",
    "# After the forward propagation it's time for\n",
    "# The creme de la creme of the implementation - Backward Propagation\n",
    "\n",
    "# For the input we will need:\n",
    "#  1) sums and activations from the forward propagation,\n",
    "#  2) the weights of the all previous layers\n",
    "#  3) input sample with corresponding output sample (just one pair of samples!)\n",
    "def backward_prop(sum_1: np.array,\n",
    "                  activate_1: np.array,\n",
    "                  sum_2: np.array,\n",
    "                  activate_2: np.array,\n",
    "                  ann: ANN,\n",
    "                  input_x: np.array,\n",
    "                  output_y: np.array) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "\n",
    "    # First, calculate the derivative of the cost function\n",
    "    # (Remember - it will always return a scalar value! )\n",
    "    cost_deriv = 2 * np.sum(activate_2 - output_y)\n",
    "\n",
    "    # Let's use it to calculate updates (derivatives) of the weights & biases in the last layer\n",
    "\n",
    "    # Studying the equations of the backprog, we can observe, that the bias term is slightly easier to implement\n",
    "    # What is more, *we can reuse it* in calculations of weights and the layer's input derivatives!\n",
    "    # NOTE: The result is a vector!\n",
    "    bias_2_deriv = softmax_deriv(sum_2) * cost_deriv\n",
    "\n",
    "    # In addition, to obtain the derivative of the weights we need just to multiply the bias deriv by the layer's input!\n",
    "    # NOTE: Using 2 vectors we want to obtain result with dimensions of weights matrix!\n",
    "    weights_2_deriv = activate_1.dot( bias_2_deriv.transpose() )\n",
    "\n",
    "    # Now, lest propagate the gradient to the first layer by calculating the derivative of the cost function in terms of the input\n",
    "    layer_2 = ann.layer_2\n",
    "    activate_1_deriv = layer_2.weights.dot( bias_2_deriv )\n",
    "\n",
    "    # We can now calculate the derivatives for the 1st layer, reusing the above logic\n",
    "    bias_1_deriv = ReLU_deriv(sum_1) * activate_1_deriv\n",
    "    weights_1_deriv = input_x.dot( bias_1_deriv.transpose() )\n",
    "    # NOTE: We don't want to calculate the derivative of the input of the wole network\n",
    "    # (i.e. we don't want to pass the gradient to the dataset)\n",
    "\n",
    "    # Return calculated updates (derivaites) for the weights and biases\n",
    "    return weights_1_deriv, bias_1_deriv, weights_2_deriv, bias_2_deriv\n",
    "\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Now lets define function for updating network parameters - weights & biases\n",
    "# The alpha paramaeters is a learning rate\n",
    "# Let's update the params in naive way for further simplification\n",
    "def update_params(ann: ANN,\n",
    "                  weights_1_deriv: np.array,\n",
    "                  bias_1_deriv: np.array,\n",
    "                  weights_2_deriv: np.array,\n",
    "                  bias_2_deriv: np.array,\n",
    "                  alpha: float) -> ANN:\n",
    "    layer_1 = ann.layer_1\n",
    "    layer_1.weights = layer_1.weights - alpha * weights_1_deriv\n",
    "    layer_1.bias = layer_1.bias - alpha * bias_1_deriv\n",
    "\n",
    "    layer_2 = ann.layer_2\n",
    "    layer_2.weights = layer_2.weights - alpha * weights_2_deriv\n",
    "    layer_2.bias = layer_2.bias - alpha * bias_2_deriv\n",
    "\n",
    "    # Return the ANN with updated weightes & biases\n",
    "    return ANN(layer_1, layer_2)\n",
    "\n",
    "# =============================== #\n",
    "\n",
    "# Finally, let's define Gradient Descent to call above functions\n",
    "def gradient_descent(input_x: np.array,\n",
    "                     output_y: np.array,\n",
    "                     alpha: float,\n",
    "                     iterations: int) -> ANN:\n",
    "\n",
    "    # First, initialize network with random parameters\n",
    "    ann = init_ann_params()\n",
    "    for _ in range(iterations):\n",
    "        # 1) Forward Propragation\n",
    "        sum_1, activate_1, sum_2, activate_2 = forward_prop(ann, input_x)\n",
    "        # 2) Backwards Propagation\n",
    "        weights_1_deriv, bias_1_deriv, weights_2_deriv, bias_2_deriv = backward_prop(sum_1,\n",
    "                                                                                     activate_1,\n",
    "                                                                                     sum_2,\n",
    "                                                                                     activate_2,\n",
    "                                                                                     ann,\n",
    "                                                                                     input_x,\n",
    "                                                                                     output_y)\n",
    "        # 3) Update parameters\n",
    "        ann = update_params(ann,\n",
    "                            weights_1_deriv,\n",
    "                            bias_1_deriv,\n",
    "                            weights_2_deriv,\n",
    "                            bias_2_deriv,\n",
    "                            alpha)\n",
    "\n",
    "    # Return the updated network\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "koOlpHWvufGP"
   },
   "source": [
    "Part 2: Basic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1eAk1oI1Szq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def activation_function(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation_function_deriv(x: float) -> float:\n",
    "    sigmoid = activation_function(x)\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size: int, act_func: Callable, act_func_deriv: Callable):\n",
    "        self._init_weights_and_bias(input_size)\n",
    "        self._activation_function = act_func\n",
    "        self._activation_function_deriv = act_func_deriv\n",
    "\n",
    "    def _init_weights_and_bias(self, input_size: int):\n",
    "        self.weights = np.random.randn(input_size, 1)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "    def __call__(self, x: np.array) -> np.array:\n",
    "        return self._forward_propagation(x)\n",
    "\n",
    "    def _forward_propagation(self, x: np.array) -> np.array:\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return self._activation_function(z).flatten()\n",
    "\n",
    "    def gradient_descent(self, x: np.array, y_target: np.array, alpha: float, iterations: int) -> None:\n",
    "        for _ in range(iterations):\n",
    "            grad_w, grad_b = self._backward_propagation(x, y_target)\n",
    "            self.weights -= alpha * grad_w\n",
    "            self.bias -= alpha * grad_b\n",
    "\n",
    "    def _backward_propagation(self, x: np.array, y: np.array) -> Tuple[np.array, np.array]:\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        y_pred = self._activation_function(z).flatten()\n",
    "\n",
    "        error = y_pred - y\n",
    "\n",
    "        grad_z = error * self._activation_function_deriv(z).flatten()\n",
    "        grad_w = np.dot(x.T, grad_z) / x.shape[0]\n",
    "        grad_b = np.mean(grad_z)\n",
    "\n",
    "        return grad_w.reshape(self.weights.shape), grad_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v44hq_Z0zFR8"
   },
   "source": [
    "## Part 3: Artificial Neuron as binary clasifier\n",
    "A single neuron used as binary classifier is also known as *perceptron*, frequently used as building block for *dense* layer. It can be used for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1nsURIZzGxk"
   },
   "source": [
    "### Task 3.1\n",
    "1) Using your Neuron class construct a following ANN:\n",
    "  * Input size: 2\n",
    "  * 1 layer with 1 unit with any activation function\n",
    "  * Output size: 1\n",
    "\n",
    "2) Perform separate trainings on provided datasets of truth tables of logic gates. You can experiment with number of iterations (start with n=500) and learnining rate (start with alpha = 0.1)\n",
    "\n",
    "3) Visualize each dataset and ANN's result (a regression line, as function of two inputs).\n",
    "\n",
    "4) Comment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eSuKn-icz-AN",
    "outputId": "4972b45f-5065-4977-a66e-184b60eb0253"
   },
   "outputs": [],
   "source": [
    "def train_and_visualize(dataset_x, dataset_y, title):\n",
    "    X = np.array(dataset_x)\n",
    "    y = np.array(dataset_y)\n",
    "\n",
    "    neuron = Neuron(input_size=2, act_func=activation_function, act_func_deriv=activation_function_deriv)\n",
    "\n",
    "    alpha = 0.1\n",
    "    iterations = 500\n",
    "    neuron.gradient_descent(X, y, alpha, iterations)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f\"{title} Gate Decision Boundary\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bwr\", marker=\"o\", edgecolor=\"k\", s=100, label=\"Data Points\")\n",
    "\n",
    "    x0_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    x1_vals = np.linspace(-0.5, 1.5, 100)\n",
    "    x0_grid, x1_grid = np.meshgrid(x0_vals, x1_vals)\n",
    "    grid_points = np.c_[x0_grid.ravel(), x1_grid.ravel()]\n",
    "\n",
    "    grid_outputs = np.array([neuron(point) for point in grid_points]).reshape(x0_grid.shape)\n",
    "\n",
    "    plt.contourf(x0_grid, x1_grid, grid_outputs, levels=[0, 0.5, 1], cmap=\"bwr\", alpha=0.3)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Input 1\")\n",
    "    plt.ylabel(\"Input 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "dataset_or_x = ((0,0), (0,1), (1,0), (1,1))\n",
    "dataset_or_y = (0, 1, 1, 1)\n",
    "\n",
    "dataset_and_x = ((0,0), (0,1), (1,0), (1,1))\n",
    "dataset_and_y = (0, 0, 0, 1)\n",
    "\n",
    "dataset_xor_x = ((0,0), (0,1), (1,0), (1,1))\n",
    "dataset_xor_y = (0, 1, 1, 0)\n",
    "\n",
    "train_and_visualize(dataset_or_x, dataset_or_y, \"OR\")\n",
    "train_and_visualize(dataset_and_x, dataset_and_y, \"AND\")\n",
    "train_and_visualize(dataset_xor_x, dataset_xor_y, \"XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mqLXsSM2CDf"
   },
   "source": [
    "**OR Gate**: The neuron learned the OR logic, as it is linearly separable. The decision boundary separates the region where at least one input is 1.\n",
    "\n",
    "**AND Gate**: The neuron also succeed with the AND gate, which is also linearly separable. The boundary include only the (1, 1) point in the \"1\" region.\n",
    "\n",
    "**XOR Gate**: The neuron struggle with XOR, as XOR is not linearly separable.\n",
    "A single-layer neuron cannot accurately model XOR, so the decision boundary will be inadequate. For XOR, a multi-layer network is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EISMH_Te2dx4"
   },
   "source": [
    "## Part 4: Multilayer perceptron\n",
    "More neurons can be stacked together to model nonlinear properties.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N10TjQ5O2ih_"
   },
   "source": [
    "### Task 4.1\n",
    "In this task you have to implement following ANN:\n",
    "* Input size: 2\n",
    "* 1 layer with 2 units with sigmoid activation function\n",
    "* 1 layer with 1 unit with sigmoid activation function\n",
    "* Output size: 1\n",
    "    \n",
    "Your Neuron class was not designed for ambitious merging of weights and biases during the gradient descent, nor for passing outputs to perform forward propagation. To overcome such inconvenience, please manually define dataflow and method calling for all Neurons. You can expand provided example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzyGpVD-2eGE"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_size: int, act_func: Callable, act_func_deriv: Callable):\n",
    "        self._neuron_1 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_2 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_3 = Neuron(input_size, act_func, act_func_deriv)\n",
    "\n",
    "    def __call__(self, x: np.array) -> float:\n",
    "        return self._network_forward_propagation(x)\n",
    "\n",
    "    def _network_forward_propagation(self, x: np.array) -> float:\n",
    "        input_3_1 = self._neuron_1(x)\n",
    "        input_3_2 = self._neuron_2(x)\n",
    "        input_3 = np.column_stack((input_3_1, input_3_2))\n",
    "        return self._neuron_3(input_3)\n",
    "\n",
    "    def _network_backwards_propagation(self, x: np.array, y: np.array) -> Tuple[Tuple[np.array, np.array], Tuple[np.array, np.array], Tuple[np.array, np.array]]:\n",
    "        input_3_1 = self._neuron_1(x)\n",
    "        input_3_2 = self._neuron_2(x)\n",
    "        input_3 = np.column_stack((input_3_1, input_3_2))\n",
    "        y_pred = self._neuron_3(input_3)\n",
    "\n",
    "        error_3 = y_pred - y\n",
    "\n",
    "        grad_w_3, grad_b_3 = self._neuron_3._backward_propagation(input_3, y)\n",
    "\n",
    "        grad_z_3 = error_3 * self._neuron_3._activation_function_deriv(np.dot(input_3, self._neuron_3.weights) + self._neuron_3.bias).flatten()\n",
    "        grad_input_3_1 = grad_z_3 * self._neuron_3.weights[0]\n",
    "        grad_input_3_2 = grad_z_3 * self._neuron_3.weights[1]\n",
    "\n",
    "        grad_w_1, grad_b_1 = self._neuron_1._backward_propagation(x, grad_input_3_1)\n",
    "        grad_w_2, grad_b_2 = self._neuron_2._backward_propagation(x, grad_input_3_2)\n",
    "\n",
    "        return (grad_w_1, grad_b_1), (grad_w_2, grad_b_2), (grad_w_3, grad_b_3)\n",
    "\n",
    "    def gradient_descent(self, x: np.array, y: np.array, alpha: float, iterations: int) -> None:\n",
    "        for _ in range(iterations):\n",
    "            (grad_w_1, grad_b_1), (grad_w_2, grad_b_2), (grad_w_3, grad_b_3) = self._network_backwards_propagation(x, y)\n",
    "\n",
    "            self._neuron_1.weights -= alpha * grad_w_1\n",
    "            self._neuron_1.bias -= alpha * grad_b_1\n",
    "\n",
    "            self._neuron_2.weights -= alpha * grad_w_2\n",
    "            self._neuron_2.bias -= alpha * grad_b_2\n",
    "\n",
    "            self._neuron_3.weights -= alpha * grad_w_3\n",
    "            self._neuron_3.bias -= alpha * grad_b_3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHmf67kH2nOR"
   },
   "source": [
    "### Task 4.2\n",
    "1) Train your ANN created in task 4.1 on the XOR dataset. You can experiment with number of iterations (start with n=500) and learning rate (start with alpha=0.1).\n",
    "\n",
    "2) Visualize the dataset and ANN's result (a regression line, as function of two inputs).\n",
    "\n",
    "3) Comment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Fo18NG1p3Yhj",
    "outputId": "fdcb5e20-75d0-4558-dca1-7e5515111f37"
   },
   "outputs": [],
   "source": [
    "dataset_xor_x = ((0,0), (0,1), (1,0), (1,1))\n",
    "dataset_xor_y = (0, 1, 1, 0)\n",
    "\n",
    "X = np.array(dataset_xor_x)\n",
    "y = np.array(dataset_xor_y)\n",
    "\n",
    "neuron = NeuralNetwork(input_size=2, act_func=activation_function, act_func_deriv=activation_function_deriv)\n",
    "\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "neuron.gradient_descent(X, y, alpha, iterations)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.title(\"XOR Gate Decision Boundary\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"bwr\", marker=\"o\", edgecolor=\"k\", s=100, label=\"Data Points\")\n",
    "\n",
    "x0_vals = np.linspace(-0.5, 1.5, 100)\n",
    "x1_vals = np.linspace(-0.5, 1.5, 100)\n",
    "x0_grid, x1_grid = np.meshgrid(x0_vals, x1_vals)\n",
    "grid_points = np.c_[x0_grid.ravel(), x1_grid.ravel()]\n",
    "\n",
    "grid_outputs = np.array([neuron(point) for point in grid_points]).reshape(x0_grid.shape)\n",
    "\n",
    "plt.contourf(x0_grid, x1_grid, grid_outputs, levels=[0, 0.5, 1], cmap=\"bwr\", alpha=0.3)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Input 1\")\n",
    "plt.ylabel(\"Input 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9adlzAht7zzI"
   },
   "source": [
    "The neural networks works better but still not good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FQsW0zT4i7vH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size: int, activation_function: Callable, activation_function_deriv: Callable):\n",
    "        self._input_size = input_size\n",
    "        self._activation_function = activation_function\n",
    "        self._activation_function_deriv = activation_function_deriv\n",
    "        self._init_weights_and_bias()\n",
    "\n",
    "    def _init_weights_and_bias(self):\n",
    "        # Initialize weights and biases randomly\n",
    "        self._weights = np.random.randn(self._input_size)\n",
    "        self._bias = np.random.randn()\n",
    "\n",
    "    def __call__(self, x: np.array) -> np.array:\n",
    "        return self._forward_propagation(x)[1]  # Return the activated output only\n",
    "\n",
    "    def _forward_propagation(self, x: np.array):\n",
    "        # Linear transformation\n",
    "        z = np.dot(self._weights, x) + self._bias\n",
    "        # Activation\n",
    "        a = self._activation_function(z)\n",
    "        return z, a  # Return both linear output and activated output for backpropagation\n",
    "\n",
    "    def _backward_propagation(self, dz: float, x: np.array):\n",
    "        # Compute gradients of weights and bias\n",
    "        dw = dz * x\n",
    "        db = dz\n",
    "        return dw, db\n",
    "\n",
    "    def _update_weights_and_bias(self, dw: np.array, db: float, alpha: float):\n",
    "        # Gradient descent step\n",
    "        self._weights -= alpha * dw\n",
    "        self._bias -= alpha * db\n",
    "\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, input_size: int, act_func: Callable, act_func_deriv: Callable):\n",
    "        # Initialize neurons\n",
    "        self._neuron_1 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_2 = Neuron(input_size, act_func, act_func_deriv)\n",
    "        self._neuron_3 = Neuron(2, act_func, act_func_deriv)\n",
    "\n",
    "    def __call__(self, x: np.array) -> float:\n",
    "        return self._network_forward_propagation(x)\n",
    "\n",
    "    def _network_forward_propagation(self, x: np.array) -> float:\n",
    "        input_3_1 = self._neuron_1(x)\n",
    "        input_3_2 = self._neuron_2(x)\n",
    "        input_3 = np.array([input_3_1, input_3_2])\n",
    "        return self._neuron_3(input_3)\n",
    "\n",
    "    def _network_backwards_propagation(self, x: np.array, y: float, alpha: float) -> None:\n",
    "        # Forward pass to get outputs\n",
    "        z1, a1 = self._neuron_1._forward_propagation(x)\n",
    "        z2, a2 = self._neuron_2._forward_propagation(x)\n",
    "        input_3 = np.array([a1, a2])\n",
    "        z3, a3 = self._neuron_3._forward_propagation(input_3)\n",
    "\n",
    "        # Calculate output error\n",
    "        error_3 = a3 - y\n",
    "        dz3 = error_3 * self._neuron_3._activation_function_deriv(z3)\n",
    "\n",
    "        # Gradients for neuron 3\n",
    "        dw3, db3 = dz3 * input_3, dz3\n",
    "        self._neuron_3._update_weights_and_bias(dw3, db3, alpha)\n",
    "\n",
    "        # Backpropagate to neuron 1 and neuron 2\n",
    "        dz1 = dz3 * self._neuron_3._weights[0] * self._neuron_1._activation_function_deriv(z1)\n",
    "        dw1, db1 = dz1 * x, dz1\n",
    "        self._neuron_1._update_weights_and_bias(dw1, db1, alpha)\n",
    "\n",
    "        dz2 = dz3 * self._neuron_3._weights[1] * self._neuron_2._activation_function_deriv(z2)\n",
    "        dw2, db2 = dz2 * x, dz2\n",
    "        self._neuron_2._update_weights_and_bias(dw2, db2, alpha)\n",
    "\n",
    "    def gradient_descent(self, x: np.array, y: float, alpha: float, iterations: int) -> None:\n",
    "        for _ in range(iterations):\n",
    "              self._network_backwards_propagation(x, y, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G2uKQVTi05x"
   },
   "outputs": [],
   "source": [
    "def train_nn(nn: NeuralNetwork, dataset_x: np.array, dataset_y: np.array, alpha: float, epochs: int) -> None:\n",
    "    for _ in range(epochs):\n",
    "        for x, y in zip(dataset_x, dataset_y):\n",
    "            x = np.array(x)\n",
    "            nn.gradient_descent(x, y, alpha, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from typing import Callable\n",
    "\n",
    "# Assuming NeuralNetwork and related classes are already defined\n",
    "\n",
    "def train_nn_with_logging(nn: NeuralNetwork, \n",
    "                          dataset_x: np.array, \n",
    "                          dataset_y: np.array, \n",
    "                          alpha: float, \n",
    "                          epochs: int, \n",
    "                          validation_split: float = 0.1):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"neural-network-training\", name=\"basic-nn-training\", config={\n",
    "        \"alpha\": alpha,\n",
    "        \"epochs\": epochs,\n",
    "        \"validation_split\": validation_split,\n",
    "        \"input_size\": len(dataset_x[0]),\n",
    "    })\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    split_index = int(len(dataset_x) * (1 - validation_split))\n",
    "    train_x, val_x = dataset_x[:split_index], dataset_x[split_index:]\n",
    "    train_y, val_y = dataset_y[:split_index], dataset_y[split_index:]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        training_loss = 0.0\n",
    "        for x, y in zip(train_x, train_y):\n",
    "            x = np.array(x)\n",
    "            nn.gradient_descent(x, y, alpha, 10)\n",
    "            prediction = nn(x)\n",
    "            training_loss += (prediction - y) ** 2\n",
    "\n",
    "        training_loss /= len(train_x)\n",
    "\n",
    "        # Validation\n",
    "        validation_loss = 0.0\n",
    "        if len(val_x) > 0:\n",
    "            for x, y in zip(val_x, val_y):\n",
    "                prediction = nn(np.array(x))\n",
    "                validation_loss += (prediction - y) ** 2\n",
    "\n",
    "            validation_loss /= len(val_x)\n",
    "\n",
    "        # Log metrics\n",
    "        metrics = {\"training_loss\": training_loss}\n",
    "        if len(val_x) > 0:\n",
    "            metrics[\"validation_loss\"] = validation_loss\n",
    "\n",
    "        wandb.log(metrics, step=epoch)\n",
    "\n",
    "    # Save code as an artifact\n",
    "    artifact = wandb.Artifact(\"nn_training_code\", type=\"code\")\n",
    "    artifact.add_file(\"(Re)Introduction_to_Neural_Networks.ipynb\")  # Replace with your script's filename\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tomasz/Private/UST/MGR/semester-2/advanced-ml/mldevops_exercise/mldevops_exercise/wandb/run-20241119_005120-qd9bz8ii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training/runs/qd9bz8ii' target=\"_blank\">basic-nn-training</a></strong> to <a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training' target=\"_blank\">https://wandb.ai/tom-snow-wind-agh/neural-network-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training/runs/qd9bz8ii' target=\"_blank\">https://wandb.ai/tom-snow-wind-agh/neural-network-training/runs/qd9bz8ii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa0166d17a48f28c5372ceb759d8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>█▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>training_loss</td><td>0.08444</td></tr><tr><td>validation_loss</td><td>0.12511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basic-nn-training</strong> at: <a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training/runs/qd9bz8ii' target=\"_blank\">https://wandb.ai/tom-snow-wind-agh/neural-network-training/runs/qd9bz8ii</a><br/> View project at: <a href='https://wandb.ai/tom-snow-wind-agh/neural-network-training' target=\"_blank\">https://wandb.ai/tom-snow-wind-agh/neural-network-training</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_005120-qd9bz8ii/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example data\n",
    "dataset_x = np.random.rand(100, 3)  # 100 samples, 3 features\n",
    "dataset_y = np.random.rand(100)    # 100 target values\n",
    "\n",
    "# Define activation functions\n",
    "def activation_function(x: float) -> float:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation_function_deriv(x: float) -> float:\n",
    "    sigmoid = activation_function(x)\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "nn = NeuralNetwork(input_size=3, act_func=activation_function, act_func_deriv=activation_function_deriv)\n",
    "\n",
    "train_nn_with_logging(nn, dataset_x, dataset_y, alpha=0.01, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C45Wm_YJKkr4"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(nn: NeuralNetwork, X: np.array, Y: np.array):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    predictions = np.array([nn(np.array(point)) for point in grid])\n",
    "    predictions = predictions.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, predictions, levels=[0, 0.5, 1], cmap=\"coolwarm\", alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolor=\"k\", cmap=\"coolwarm\", s=100)\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(\"Decision Boundary of Neural Network for XOR Problem\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "DkDukiM3lgVo",
    "outputId": "3b17e1b3-fa91-44ae-d709-71ff5da0b21b"
   },
   "outputs": [],
   "source": [
    "dataset_xor_x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "dataset_xor_y = np.array([0, 1, 1, 0])\n",
    "\n",
    "neuron = NeuralNetwork(input_size=2, act_func=activation_function, act_func_deriv=activation_function_deriv)\n",
    "\n",
    "alpha = 0.01\n",
    "iterations = 10000\n",
    "epochs = 10000\n",
    "\n",
    "train_nn(neuron,dataset_xor_x, dataset_xor_y,  alpha, epochs)\n",
    "plot_decision_boundary(neuron, dataset_xor_x, dataset_xor_y)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
